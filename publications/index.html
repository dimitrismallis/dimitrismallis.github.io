<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="lhGajE3afrJp3Jyzmt5y_513I9mjrw1rCg9KE9PQBY0">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Dr. Dimitri(o)s  Mallis</title>
    <meta name="author" content="Dr. Dimitri(o)s  Mallis">
    <meta name="description" content="List of publications in reversed chronological order.">
    <meta name="keywords" content="dimitrismallis, mallis, dimitriosmallis">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%95%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://dimitrismallis.github.io//publications/">
    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Dr. Dimitri(o)s </span>Mallis</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">Vitae</a>
              </li>
<!-- Photography porfolio link -->
              <li class="nav-item">
                <a class="nav-link" href="https://dimitrismallis.myportfolio.com/" rel="external nofollow noopener" target="_blank">Photography
                <span class="sr-only">(current)</span></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">List of publications in reversed chronological order.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/davinci-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/davinci-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/davinci-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/davinci.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="davinci.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Karadeniz2024PICASSOAF" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://cvi2snt.github.io/davinci/" style="color: black;" rel="external nofollow noopener" target="_blank"> DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch Inference </a></div>
        <!-- Author -->
        <div class="author">
        

        Ahmet Serdar Karadeniz, <em>Dimitrios Mallis</em>, Nesryne Mejri, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>British Machine Vision Conference (BMVC)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2410.22857" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://cvi2snt.github.io/davinci/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="https://github.com/cvi2snt/CPTSketchGraphs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This work presents DAVINCI, a unified architecture for single-stage Computer-Aided Design (CAD) sketch parameterization and constraint inference directly from raster sketch images. By jointly learning both outputs, DAVINCI minimizes error accumulation and enhances the performance of constrained CAD sketch inference. Notably, DAVINCI achieves state-of-the-art results on the large-scale SketchGraphs dataset, demonstrating effectiveness on both precise and hand-drawn raster CAD sketches. To reduce DAVINCI’s reliance on large-scale annotated datasets, we explore the efficacy of CAD sketch augmentations. We introduce Constraint-Preserving Transformations (CPTs), i.e. random permutations of the parametric primitives of a CAD sketch that preserve its constraints. This data augmentation strategy allows DAVINCI to achieve reasonable performance when trained with only 0.1% of the SketchGraphs dataset. Furthermore, this work contributes a new version of SketchGraphs, augmented with CPTs. The newly introduced CPTSketchGraphs dataset includes 80 million CPT-augmented sketches, thus providing a rich resource for future research in the CAD sketch domain.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Karadeniz2024PICASSOAF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch Inference}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karadeniz, Ahmet Serdar and Mallis, Dimitrios and Mejri, Nesryne and Cherenkova, Kseniya and Kacem, Anis and Aouada, Djamila}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{British Machine Vision Conference (BMVC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/picasso-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/picasso-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/picasso-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/picasso.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="picasso.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Karadeniz2025DAVINCI" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://cvi2snt.github.io/picasso/" style="color: black;" rel="external nofollow noopener" target="_blank"> PICASSO: A Feed-Forward Framework for Parametric Inference of CAD Sketches via Rendering Self-Supervision </a></div>
        <!-- Author -->
        <div class="author">
        

        Ahmet Serdar Karadeniz, <em>Dimitrios Mallis</em>, Nesryne Mejri, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Arxiv (now accepted to WACV)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.13394" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://cvi2snt.github.io/picasso/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This work introduces PICASSO, a framework for the parameterization of 2D CAD sketches from hand-drawn and precise sketch images. PICASSO converts a given CAD sketch image into parametric primitives that can be seamlessly integrated into CAD software. Our framework leverages rendering self-supervision to enable the pre-training of a CAD sketch parameterization network using sketch renderings only, thereby eliminating the need for corresponding CAD parameterization. Thus, we significantly reduce reliance on parameter-level annotations, which are often unavailable, particularly for hand-drawn sketches. The two primary components of PICASSO are (1) a Sketch Parameterization Network (SPN) that predicts a series of parametric primitives from CAD sketch images, and (2) a Sketch Rendering Network (SRN) that renders parametric CAD sketches in a differentiable manner and facilitates the computation of a rendering (image-level) loss for self-supervision. We demonstrate that the proposed PICASSO can achieve reasonable performance even when finetuned with only a small number of parametric CAD sketches. Extensive evaluation on the widely used SketchGraphs and CAD as Language datasets validates the effectiveness of the proposed approach on zero- and few-shot learning scenarios.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Karadeniz2025DAVINCI</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PICASSO: A Feed-Forward Framework for Parametric Inference of CAD Sketches via Rendering Self-Supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karadeniz, Ahmet Serdar and Mallis, Dimitrios and Mejri, Nesryne and Cherenkova, Kseniya and Kacem, Anis and Aouada, Djamila}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv (now accepted to WACV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eccv2024-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eccv2024-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eccv2024-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/eccv2024.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="eccv2024.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="dupont2024transcad" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://cvi2snt.github.io/transcad/" style="color: black;" rel="external nofollow noopener" target="_blank"> TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds </a></div>
        <!-- Author -->
        <div class="author">
        

        Elona Dupont, Kseniya Cherenkova, <em>Dimitrios Mallis</em>, Gleb Gusev, Anis Kacem, and Djamila Aouada</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>European Conference on Computer Vision (ECCV)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.12702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://cvi2snt.github.io/transcad/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>3D reverse engineering, in which a CAD model is inferred given a 3D scan of a physical object, is a research direction that offers many promising practical applications. This paper proposes TransCAD, an end-to-end transformer-based architecture that predicts the CAD sequence from a point cloud. TransCAD leverages the structure of CAD sequences by using a hierarchical learning strategy. A loop refiner is also introduced to regress sketch primitive parameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show that TransCAD achieves state-of-the-art results. The result analysis is supported with a proposed metric for CAD sequence, the mean Average Precision of CAD Sequence, that addresses the limitations of existing metrics. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">dupont2024transcad</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dupont, Elona and Cherenkova, Kseniya and Mallis, Dimitrios and Gusev, Gleb and Kacem, Anis and Aouada, Djamila}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/WACV2023_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/WACV2023_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/WACV2023_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/WACV2023_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="WACV2023_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Anastasakis2023WACV" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="" style="color: black;"> Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction </a></div>
        <!-- Author -->
        <div class="author">
        

        Zacharias Anastasakis, <em>Dimitrios Mallis</em>, Markos Diomataris, George Alexandridis, Stefanos Kollias, and Vassilis Pitsikalis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2311.04834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Anastasakis2023WACV</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Anastasakis, Zacharias and Mallis, Dimitrios and Diomataris, Markos and Alexandridis, George and Kollias, Stefanos and Pitsikalis, Vassilis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Winter Conference on Applications of Computer Vision (WACV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ICCVW2023_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ICCVW2023_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ICCVW2023_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/ICCVW2023_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ICCVW2023_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Mallis2023SHARPC2" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://cvi2.uni.lu/sharp2023-challenge/" style="color: black;" rel="external nofollow noopener" target="_blank"> SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>, Sk Aziz Ali, Elona Dupont, Kseniya Cherenkova, Ahmet Serdar Karadeniz, Mohammad Sadil Khan, Anis Kacem, Gleb Gusev, and Djamila Aouada</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>International Conference on Computer Vision Workshops (ICCVW)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2308.15966" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://cvi2.uni.lu/sharp2023-challenge/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent breakthroughs in geometric Deep Learning (DL) and the availability of large Computer-Aided Design (CAD) datasets have advanced the research on learning CAD modeling processes and relating them to real objects. In this context, 3D reverse engineering of CAD models from 3D scans is considered to be one of the most sought-after goals for the CAD industry. However, recent efforts assume multiple simplifications limiting the applications in real-world settings. The SHARP Challenge 2023 aims at pushing the research a step closer to the real-world scenario of CAD reverse engineering through dedicated datasets and tracks. In this paper, we define the proposed SHARP 2023 tracks, describe the provided datasets, and propose a set of baseline methods along with suitable evaluation metrics to assess the performance of the track solutions. All proposed datasets along with useful routines and the evaluation metrics are publicly available.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mallis2023SHARPC2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SHARP Challenge 2023: Solving CAD History and pArameters Recovery from Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios and Ali, Sk Aziz and Dupont, Elona and Cherenkova, Kseniya and Karadeniz, Ahmet Serdar and Khan, Mohammad Sadil and Kacem, Anis and Gusev, Gleb and Aouada, Djamila}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision Workshops (ICCVW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ICIP2023_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ICIP2023_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ICIP2023_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/ICIP2023_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ICIP2023_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Parelli2023InterpretableVQ" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://ieeexplore.ieee.org/abstract/document/10223156" style="color: black;" rel="external nofollow noopener" target="_blank"> Interpretable Visual Question Answering via Reasoning Supervision </a></div>
        <!-- Author -->
        <div class="author">
        

        Maria Parelli, <em>Dimitrios Mallis</em>, Markos Diomataris, and Vassilis Pitsikalis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>International Conference on Image Processing (ICIP)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2309.03726" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10223156" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Transformer-based architectures have recently demonstrated remarkable performance in the Visual Question Answering (VQA) task. However, such models are likely to disregard crucial visual cues and often rely on multimodal shortcuts and inherent biases of the language modality to predict the correct answer, a phenomenon commonly referred to as lack of visual grounding. In this work, we alleviate this shortcoming through a novel architecture for visual question answering that leverages common sense reasoning as a supervisory signal. Reasoning supervision takes the form of a textual justification of the correct answer, with such annotations being already available on large-scale Visual Common Sense Reasoning (VCR) datasets. The model’s visual attention is guided toward important elements of the scene through a similarity loss that aligns the learned attention distributions guided by the question and the correct reasoning. We demonstrate both quantitatively and qualitatively that the proposed approach can boost the model’s visual perception capability and lead to performance increase, without requiring training on explicit grounding annotations.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Parelli2023InterpretableVQ</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpretable Visual Question Answering via Reasoning Supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parelli, Maria and Mallis, Dimitrios and Diomataris, Markos and Pitsikalis, Vassilis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Image Processing (ICIP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/TPAMI2023_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/TPAMI2023_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/TPAMI2023_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/TPAMI2023_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="TPAMI2023_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Mallis2023landm" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://ieeexplore.ieee.org/document/10005822" style="color: black;" rel="external nofollow noopener" target="_blank"> From Keypoints to Object Landmarks via Self-Training Correspondence: A novel approach to Unsupervised Landmark Discovery </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>, Enrique Sanchez, Matt Bell, and Georgios Tzimiropoulos</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2205.15895" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/10005822" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="https://github.com/dimitrismallis/KeypointsToLandmarks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a novel paradigm for the unsupervised learning of object landmark detectors. Contrary to existing methods that build on auxiliary tasks such as image generation or equivariance, we propose a self-training approach where, departing from generic keypoints, a landmark detector and descriptor is trained to improve itself, tuning the keypoints into distinctive landmarks. To this end, we propose an iterative algorithm that alternates between producing new pseudo-labels through feature clustering and learning distinctive features for each pseudo-class through contrastive learning. With a shared backbone for the landmark detector and descriptor, the keypoint locations progressively converge to stable landmarks, filtering those less stable. Compared to previous works, our approach can learn points that are more flexible in terms of capturing large viewpoint changes. We validate our method on a variety of difficult datasets, including LS3D, BBCPose, Human3.6M and PennAction, achieving new state of the art results.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mallis2023landm</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios and Sanchez, Enrique and Bell, Matt and Tzimiropoulos, Georgios}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Keypoints to Object Landmarks via Self-Training Correspondence: A novel approach to Unsupervised Landmark Discovery}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/RECSYS2022_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/RECSYS2022_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/RECSYS2022_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/RECSYS2022_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="RECSYS2022_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Katsileros2022AnIL" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://dl.acm.org/doi/10.1145/3523227.3547390" style="color: black;" rel="external nofollow noopener" target="_blank"> An Incremental Learning framework for Large-scale CTR Prediction </a></div>
        <!-- Author -->
        <div class="author">
        

        Petros Katsileros, Nikiforos Mandilaras, <em>Dimitrios Mallis</em>, Vassilis Pitsikalis, Stavros Theodorakis, and Gil Chamiel</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Conference on Recommender Systems</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.00458" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/10.1145/3523227.3547390" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work we introduce an incremental learning framework for Click-Through-Rate (CTR) prediction and demonstrate its effectiveness for Taboola’s massive-scale recommendation service. Our approach enables rapid capture of emerging trends through warm-starting from previously deployed models and fine tuning on "fresh" data only. Past knowledge is maintained via a teacher-student paradigm, where the teacher acts as a distillation technique, mitigating the catastrophic forgetting phenomenon. Our incremental learning framework enables significantly faster training and deployment cycles (x12 speedup). We demonstrate a consistent Revenue Per Mille (RPM) lift over multiple traffic segments and a significant CTR increase on newly introduced items.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Katsileros2022AnIL</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Incremental Learning framework for Large-scale CTR Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Katsileros, Petros and Mandilaras, Nikiforos and Mallis, Dimitrios and Pitsikalis, Vassilis and Theodorakis, Stavros and Chamiel, Gil}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Conference on Recommender Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/THESIS_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/THESIS_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/THESIS_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/THESIS_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="THESIS_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Mallisthesis" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="" style="color: black;"> Unsupervised Landmark Discovery via Self-Training Correspondence </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>University of Nottingham</em>, 2022
        </div>
        <div class="periodical">
          PhD Thesis
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/Thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Object parts, also known as landmarks, convey information about an object’s shape and spatial configuration in 3D space, especially for deformable objects. The goal of landmark detection is to have a model that, for a particular object instance, can estimate the locations of its parts. Research in this field is mainly driven by supervised approaches, where a sufficient amount of human-annotated data is available. As annotating landmarks for all objects is impractical, this thesis focuses on learning landmark detectors without supervision. Despite good performance on limited scenarios (objects showcasing minor rigid deformation), unsupervised landmark discovery mostly remains an open problem. Existing work fails to capture semantic landmarks, i.e. points similar to the ones assigned by human annotators and may not generalise well to highly articulated objects like the human body, complicated backgrounds or large viewpoint variations.

In this thesis, we propose a novel self-training framework for the discovery of unsupervised landmarks. Contrary to existing methods that build on auxiliary tasks such as image generation or equivariance, we depart from generic keypoints and train a landmark detector and descriptor to improve itself, tuning the keypoints into distinctive landmarks. We propose an iterative algorithm that alternates between producing new pseudo-labels through feature clustering and learning distinctive features for each pseudo-class through contrastive learning. Our detector can discover highly semantic landmarks, that are more flexible in terms of capturing large viewpoint changes and out-of-plane rotations (3D rotations). New state-of-the-art performance is achieved in multiple challenging datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Mallisthesis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Landmark Discovery via Self-Training Correspondence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of Nottingham}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{PhD Thesis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/NEURIPS2020_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/NEURIPS2020_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/NEURIPS2020_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/NEURIPS2020_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="NEURIPS2020_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="unsupervLandm2020" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://proceedings.neurips.cc/paper/2020/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html" style="color: black;" rel="external nofollow noopener" target="_blank"> Unsupervised Learning of Object Landmarks via Self-Training Correspondence </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>, Enrique Sanchez, Matt Bell, and Georgios Tzimiropoulos</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Neural Information Processing Systems</em>, 2020
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://proceedings.neurips.cc/paper/2020/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="https://proceedings.neurips.cc/paper/2020/file/32508f53f24c46f685870a075eaaa29c-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/32508f53f24c46f685870a075eaaa29c-Supplemental.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a>
            <a href="https://github.com/dimitrismallis/UnsupervisedLandmarks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper addresses the problem of unsupervised discovery of object landmarks.
We take a different path compared to existing works, based on 2 novel perspectives: <b>(1) Self-training</b>: starting from generic keypoints, we propose a self-training
approach where the goal is to learn a detector that improves itself, becoming
more and more tuned to object landmarks. <b>(2) Correspondence</b>: we identify correspondence as a key objective for unsupervised landmark discovery and propose
an optimization scheme which alternates between recovering object landmark
correspondence across different images via clustering and learning an object landmark descriptor without labels. Compared to previous works, our approach can
learn landmarks that are more flexible in terms of capturing large changes in
viewpoint. We show the favourable properties of our method on a variety of difficult datasets including LS3D, BBCPose and Human3.6M.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">unsupervLandm2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Learning of Object Landmarks via Self-Training Correspondence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios and Sanchez, Enrique and Bell, Matt and Tzimiropoulos, Georgios}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Landbauforschung2019_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Landbauforschung2019_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Landbauforschung2019_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/Landbauforschung2019_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Landbauforschung2019_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="emmissionstudy" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="" style="color: black;"> Modified approach to estimating daily methane emissions of dairy cows by measuring filtered eructations during milking </a></div>
        <!-- Author -->
        <div class="author">
        

        Matt Bell, Phil Garnsworthy, <em>Dimitrios Mallis</em>, Richrd Eckard, Peter Moate, and Tianhai Yan</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Landbauforschung</em>, 2019
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/methanestudy.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The aim of this study was to compare metrics for quantifying
enteric methane (CH4) emissions from individual cows during
milking using frequent spot measurements and peak analysis
methods. An infrared gas analyser was used to measure the
CH4 emitted by cows, and eructation peaks were identified
using a Signal Processing Toolbox provided by Matlab. CH4
emissions were quantified by gas peak height, peak amplitude and average concentration, and were expressed in
grams per day and CH4 yield (grams per kilogram of dry matter intake (DMI)). Peak analysis measurements of CH4 were
obtained from 36 cows during 2,474 milkings, during which
cows were fed a ration containing between 39 and 70% forage. Spot measurements of CH4 were compared to a separate
dataset of 196 chamber CH4 records from another group of
105 cows, which were fed a ration containing between 25
and 80% forage. The results showed that the metrics of CH4
peak height and CH4 peak amplitude demonstrated similar positive relationships between daily CH4 emissions and
DMI (both r=0.37), and a negative relationship between CH4
yield and DMI (r=-0.43 and -0.38 respectively) as observed in
the chamber measurements (r=0.57 for daily emissions and
r=-0.40 for CH4 yield). The CH4 metrics of peak height and
peak amplitude were highly repeatable (ranging from 0.76
to 0.81), comparable to the high repeatability of production
traits (ranging from 0.63 to 0.99) and were more repeatable
than chamber CH4 measurements (0.31 for daily emissions and
0.03 for CH4 yield). This study recommends quantifying CH4
emissions from the maximum amplitude of an eructation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">emmissionstudy</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modified approach to estimating daily methane emissions of dairy cows by measuring filtered eructations during milking}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Landbauforschung}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bell, Matt and Garnsworthy, Phil and Mallis, Dimitrios and Eckard, Richrd and Moate, Peter and Yan, Tianhai}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/EvolvingSystems2018_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/EvolvingSystems2018_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/EvolvingSystems2018_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/EvolvingSystems2018_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="EvolvingSystems2018_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Mallis2018ConvolutiveAS" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://link.springer.com/article/10.1007/s12530-017-9199-3" style="color: black;" rel="external nofollow noopener" target="_blank"> Convolutive audio source separation using robust ICA and an intelligent evolving permutation ambiguity solution </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>, Thomas Sgouros, and Nikolaos Mitianoudis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Evolving Systems</em>, 2018
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/1708.03989" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://link.springer.com/article/10.1007/s12530-017-9199-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Audio source separation is the task of isolating sound sources that are active simultaneously in a room captured by a set of microphones. Convolutive audio source separation of equal number of sources and microphones has a number of shortcomings including the complexity of frequency-domain ICA, the permutation ambiguity and the problem’s scalabity with increasing number of sensors. In this paper, the authors propose a multiple-microphone audio source separation algorithm based on a previous work of Mitianoudis and Davies (2003). Complex FastICA is substituted by Robust ICA increasing robustness and performance. Permutation ambiguity is solved using two methodologies. The first is using the Likelihood Ration Jump solution, which is now modified to decrease computational complexity in the case of multiple microphones. The application of the MuSIC algorithm, as a preprocessing step to the previous solution, forms a second methodology with promising results.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mallis2018ConvolutiveAS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convolutive audio source separation using robust ICA and an intelligent evolving permutation ambiguity solution}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios and Sgouros, Thomas and Mitianoudis, Nikolaos}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Evolving Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/JSIT2018_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/JSIT2018_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/JSIT2018_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/JSIT2018_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="JSIT2018_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Kalamatianos2018TowardsTC" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://www.emerald.com/insight/content/doi/10.1108/JSIT-06-2017-0040/full/html" style="color: black;" rel="external nofollow noopener" target="_blank"> Towards the creation of an emotion lexicon for microblogging </a></div>
        <!-- Author -->
        <div class="author">
        

        Georgios Kalamatianos, Symeon Symeonidis, <em>Dimitrios Mallis</em>, and Avi Arampatzis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Journal of Systems and Information Technology</em>, 2018
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.emerald.com/insight/content/doi/10.1108/JSIT-06-2017-0040/full/html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="/assets/pdf/JSIT18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="http://hashtag.nonrelevant.net/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p><b>Purpose</b> - The rapid growth of social media has rendered opinion and sentiment mining an important area
of research with a wide range of applications. This paper aims to focus on the Greek language and the
microblogging platform Twitter, investigating methods for extracting emotion of individual tweets as well as
population emotion for different subjects (hashtags). <br><br>

<b>Design/methodology/approach</b> – The authors propose and investigate the use of emotion lexiconbased methods as a mean of extracting emotion/sentiment information from social media. The authors
compare several approaches for measuring the intensity of six emotions: anger, disgust, fear, happiness,
sadness and surprise. To evaluate the effectiveness of the methods, the authors develop a benchmark dataset
of tweets, manually rated by two humans. <br><br>

<b>Findings</b> – Development of a new sentiment lexicon for use in Web applications. The authors then assess
the performance of the methods with the new lexicon and find improved results.<br><br>

<b>Research limitations/implications</b> – Automated emotion results of research seem promising and
correlate to real user emotion. At this point, the authors make some interesting observations about the lexiconbased approach which lead to the need for a new, better, emotion lexicon.<br><br>

<b>Practical implications</b> – The authors examine the variation of emotion intensity over time for selected
hashtags and associate it with real-world events.<br><br>

<b>Originality/value</b> – The originality in this research is the development of a training set of tweets,
manually annotated by two independent raters. The authors “transfer” the sentiment information of these
annotated tweets, in a meaningful way, to the set of words that appear in them.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kalamatianos2018TowardsTC</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kalamatianos, Georgios and Symeonidis, Symeon and Mallis, Dimitrios and Arampatzis, Avi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Systems and Information Technology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/AIAI2016_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/AIAI2016_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/AIAI2016_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/AIAI2016_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="AIAI2016_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Mallis2016ConvolutiveAS" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://www.springerprofessional.de/en/convolutive-audio-source-separation-using-robust-ica-and-reduced/10651414" style="color: black;" rel="external nofollow noopener" target="_blank"> Convolutive Audio Source Separation Using Robust ICA and Reduced Likelihood Ratio Jump </a></div>
        <!-- Author -->
        <div class="author">
        

        <em>Dimitrios Mallis</em>, Thomas Sgouros, and Nikolaos Mitianoudis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Artificial Intelligence Applications and Innovations</em>, 2016
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.springerprofessional.de/en/convolutive-audio-source-separation-using-robust-ica-and-reduced/10651414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="/assets/pdf/aiai2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Audio source separation is the task of isolating sound sources that are active simultaneously in a room captured by a set of microphones. Convolutive audio source separation of equal number of sources and microphones has a number of shortcomings including the complexity of frequency-domain ICA, the permutation ambiguity and the problem’s scalabity with increasing number of sensors. In this paper, the authors propose a multiple-microphone audio source separation algorithm based on a previous work of Mitianoudis and Davies. Complex FastICA is substituted by Robust ICA increasing robustness and performance. Permutation ambiguity is solved using the Likelihood Ration Jump solution, which is now modified to decrease computational complexity in the case of multiple microphones.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mallis2016ConvolutiveAS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convolutive Audio Source Separation Using Robust ICA and Reduced Likelihood Ratio Jump}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mallis, Dimitrios and Sgouros, Thomas and Mitianoudis, Nikolaos}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Artificial Intelligence Applications and Innovations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2"><div class="preview">
              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/PCI2015_preview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/PCI2015_preview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/PCI2015_preview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/PCI2015_preview.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="PCI2015_preview.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div></div>

        <!-- Entry bib key -->
        <div id="Kalamatianos2015SentimentAO" class="col-sm-8">
        <!-- Title -->
        <div class="title"><a href="https://dl.acm.org/doi/10.1145/2801948.2802010" style="color: black;" rel="external nofollow noopener" target="_blank"> Sentiment analysis of greek tweets and hashtags using a sentiment lexicon </a></div>
        <!-- Author -->
        <div class="author">
        

        Georgios Kalamatianos, <em>Dimitrios Mallis</em>, Symeon Symeonidis, and Avi Arampatzis</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Panhellenic Conference on Informatics</em>, 2015
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/10.1145/2801948.2802010" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
            <a href="/assets/pdf/PCI2015a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="http://hashtag.nonrelevant.net/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1145/2801948.2802010"></span>
              <span class="__dimensions_badge_embed__" data-doi="10.1145/2801948.2802010" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The rapid growth of social media has rendered opinion and
sentiment mining an important area of research with a wide range
of applications. We focus on the Greek language and the
microblogging platform “Twitter”, investigating methods for
extracting sentiment of individual tweets as well population
sentiment for different subjects (hashtags). The proposed methods
are based on a sentiment lexicon. We compare several approaches
for measuring the intensity of “Anger”, “Disgust”, “Fear”,
“Happiness”, “Sadness”, and “Surprise”. To evaluate the
effectiveness of our methods, we develop a benchmark dataset of
tweets, manually rated by two humans. Our automated sentiment
results seem promising and correlate to real user sentiment. Finally,
we examine the variation of sentiment intensity over time for
selected hashtags, and associate it with real-world events.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kalamatianos2015SentimentAO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sentiment analysis of greek tweets and hashtags using a sentiment lexicon}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kalamatianos, Georgios and Mallis, Dimitrios and Symeonidis, Symeon and Arampatzis, Avi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Panhellenic Conference on Informatics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2801948.2802010}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Dr. Dimitri(o)s  Mallis. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-NDX68PFQ2J"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-NDX68PFQ2J');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
